{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "  root='./dataset/', train=True, transform=transform, download=True\n",
    ")\n",
    "\n",
    "test_dataset = datasets.MNIST(\n",
    "  root='./dataset/', train=False, transform=transform, download=True\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "  train_dataset, batch_size=100, shuffle=True, num_workers=4\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "  test_dataset, batch_size=100, shuffle=False, num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.input1 = nn.Linear(input_dim, hidden_dim)\n",
    "    self.input2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "    self.mean = nn.Linear(hidden_dim, latent_dim)\n",
    "    self.var = nn.Linear(hidden_dim, latent_dim)\n",
    "    \n",
    "    self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "    self.training = True\n",
    "    \n",
    "  def forward(self, x):\n",
    "    h_ = self.LeakyReLU(self.input1(x))\n",
    "    h_ = self.LeakyReLU(self.input2(h_))\n",
    "    mean = self.mean(h_)\n",
    "    log_var = self.var(h_)\n",
    "    return mean, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "  def __init__(self, latent_dim, hidden_dim, output_dim):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.hidden1 = nn.Linear(latent_dim, hidden_dim)\n",
    "    self.hidden2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "    self.output = nn.Linear(hidden_dim, output_dim)\n",
    "    self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    h = self.LeakyReLU(self.hidden1(x))\n",
    "    h = self.LeakyReLU(self.hidden2(h))\n",
    "    x_hat = torch.sigmoid(self.output(h))\n",
    "    return x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "  def __init__(self, Encoder, Decoder):\n",
    "    super(Model, self).__init__()\n",
    "    self.Encoder = Encoder\n",
    "    self.Decoder = Decoder\n",
    "   \n",
    "  # 평균과 표준 편차가 주어졌을 때, latent vector z를 만들기 위한(=sampling하기 위한) 함수\n",
    "  #  z는 gaussian 분포로 가정했기 때문에, encoder에서 받아 온 평균과 표준 편차를 이용해 z를 생성한다.\n",
    "  def reparameterization(self, mean, var):\n",
    "    epsilon = torch.rand_like(var).to(device)\n",
    "    z = mean + var * epsilon\n",
    "    return z\n",
    "  \n",
    "  def forward(self, x):\n",
    "    mean, log_var = self.Encoder(x)\n",
    "    z = self.reparameterization(mean, torch.exp(0.5 * log_var)) # 표준 편차 값이 음수가 되지 않도록 로그를 취하는 방식 사용함.\n",
    "    x_hat = self.Decoder(z)# z 벡터를 decoder에 통과시켜서, 입력과 동일한 데이터(x')을 만들어 내는 작업.\n",
    "    return x_hat, mean, log_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder, Decoder 객체 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dim = 784\n",
    "hidden_dim = 400\n",
    "latent_dim = 200\n",
    "epochs = 30\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(\n",
    "  input_dim=x_dim,\n",
    "  hidden_dim=hidden_dim,\n",
    "  latent_dim=latent_dim\n",
    ")\n",
    "\n",
    "decoder = Decoder(\n",
    "  latent_dim=latent_dim,\n",
    "  hidden_dim=hidden_dim,\n",
    "  output_dim=x_dim\n",
    ")\n",
    "\n",
    "model = Model(Encoder=encoder, Decoder=decoder).to(device)\n",
    "model = nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(x, x_hat, mean, log_var):\n",
    "  reproduction_loss = nn.functional.binary_cross_entropy(x_hat, x, reduction='sum')\n",
    "  KLD = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "  return reproduction_loss, KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_loc = './tensorboard_scalar'\n",
    "writer = SummaryWriter(saved_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "def train(epoch, model, train_loader, optimizer):\n",
    "  train_loss = 0\n",
    "  for batch_idx, (x, _) in enumerate(train_loader):\n",
    "    \n",
    "    x = x.view(batch_size, x_dim)\n",
    "    x = x.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    x_hat, mean, log_var = model(x)\n",
    "    BCE, KLD = loss_function(x, x_hat, mean, log_var)\n",
    "    loss = BCE + KLD\n",
    "    \n",
    "    writer.add_scalar(\"Train/Reconstruction Error\", BCE.item(), \n",
    "                      batch_idx + epoch * (len(train_loader.dataset)/batch_size))\n",
    "    writer.add_scalar(\"Train/KL-Divergence\", KLD.item(), \n",
    "                      batch_idx + epoch * (len(train_loader.dataset)/batch_size))\n",
    "    writer.add_scalar(\"Train/Total Loss\", loss.item(), \n",
    "                      batch_idx + epoch * (len(train_loader.dataset)/batch_size))\n",
    "    \n",
    "    train_loss += loss.item()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if batch_idx % 100 == 0:\n",
    "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\t Loss: {:.6f}'\n",
    "            .format(epoch, batch_idx*len(x), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader),\n",
    "                    loss.item() / len(x)))\n",
    "      \n",
    "      print(\"======> Epoch: {} Average Loss: {:.4f}\"\n",
    "            .format(epoch, train_loss / len(train_loader.dataset)))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, model, test_loader):\n",
    "  model.eval()\n",
    "  test_loss = 0\n",
    "  with torch.no_grad():\n",
    "    for batch_idx, (x,_) in enumerate(test_loader):\n",
    "      x = x.view(batch_size, x_dim)\n",
    "      x = x.to(device)\n",
    "      x_hat, mean, log_var = model(x)\n",
    "      BCE, KLD = loss_function(x, x_hat, mean, log_var)\n",
    "      loss = BCE + KLD\n",
    "      \n",
    "      writer.add_scalar(\"Test/Reconstruction Error\", BCE.item(), \n",
    "                      batch_idx + epoch * (len(test_loader.dataset)/batch_size))\n",
    "      writer.add_scalar(\"Test/KL-Divergence\", KLD.item(), \n",
    "                      batch_idx + epoch * (len(test_loader.dataset)/batch_size))\n",
    "      writer.add_scalar(\"Test/Total Loss\", loss.item(), \n",
    "                      batch_idx + epoch * (len(test_loader.dataset)/batch_size))\n",
    "      \n",
    "      test_loss += loss.item()\n",
    "      \n",
    "      if batch_idx == 0:\n",
    "        n = min(x.size(0), 8)\n",
    "        comparison = torch.cat([x[:n], x_hat.view(batch_size, x_dim)[:n]])\n",
    "        grid = torchvision.utils.make_grid(comparison.cpu())\n",
    "        writer.add_image(\"Test image - Above: Real data, below: reconstruction data\", grid, epoch)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fb3ddfb7a9b410199c89e6070e1378f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\t Loss: 117.073232\n",
      "======> Epoch: 0 Average Loss: 0.1951\n",
      "Train Epoch: 0 [10000/60000 (17%)]\t Loss: 112.569063\n",
      "======> Epoch: 0 Average Loss: 19.7669\n",
      "Train Epoch: 0 [20000/60000 (33%)]\t Loss: 114.339033\n",
      "======> Epoch: 0 Average Loss: 38.6350\n",
      "Train Epoch: 0 [30000/60000 (50%)]\t Loss: 106.455137\n",
      "======> Epoch: 0 Average Loss: 56.9201\n",
      "Train Epoch: 0 [40000/60000 (67%)]\t Loss: 109.703232\n",
      "======> Epoch: 0 Average Loss: 74.7653\n",
      "Train Epoch: 0 [50000/60000 (83%)]\t Loss: 103.145625\n",
      "======> Epoch: 0 Average Loss: 92.2125\n",
      "\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\t Loss: 99.267432\n",
      "======> Epoch: 1 Average Loss: 0.1654\n",
      "Train Epoch: 1 [10000/60000 (17%)]\t Loss: 95.069121\n",
      "======> Epoch: 1 Average Loss: 16.9675\n",
      "Train Epoch: 1 [20000/60000 (33%)]\t Loss: 96.321191\n",
      "======> Epoch: 1 Average Loss: 33.5499\n",
      "Train Epoch: 1 [30000/60000 (50%)]\t Loss: 100.209502\n",
      "======> Epoch: 1 Average Loss: 50.0538\n",
      "Train Epoch: 1 [40000/60000 (67%)]\t Loss: 100.019736\n",
      "======> Epoch: 1 Average Loss: 66.3872\n",
      "Train Epoch: 1 [50000/60000 (83%)]\t Loss: 98.333330\n",
      "======> Epoch: 1 Average Loss: 82.5132\n",
      "\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\t Loss: 94.349619\n",
      "======> Epoch: 2 Average Loss: 0.1572\n",
      "Train Epoch: 2 [10000/60000 (17%)]\t Loss: 88.677305\n",
      "======> Epoch: 2 Average Loss: 15.9798\n",
      "Train Epoch: 2 [20000/60000 (33%)]\t Loss: 92.013340\n",
      "======> Epoch: 2 Average Loss: 31.6462\n",
      "Train Epoch: 2 [30000/60000 (50%)]\t Loss: 94.789658\n",
      "======> Epoch: 2 Average Loss: 47.2299\n",
      "Train Epoch: 2 [40000/60000 (67%)]\t Loss: 94.348164\n",
      "======> Epoch: 2 Average Loss: 62.7632\n",
      "Train Epoch: 2 [50000/60000 (83%)]\t Loss: 88.368945\n",
      "======> Epoch: 2 Average Loss: 78.0992\n",
      "\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\t Loss: 91.120684\n",
      "======> Epoch: 3 Average Loss: 0.1519\n",
      "Train Epoch: 3 [10000/60000 (17%)]\t Loss: 91.141162\n",
      "======> Epoch: 3 Average Loss: 15.3272\n",
      "Train Epoch: 3 [20000/60000 (33%)]\t Loss: 88.253887\n",
      "======> Epoch: 3 Average Loss: 30.4414\n",
      "Train Epoch: 3 [30000/60000 (50%)]\t Loss: 90.560713\n",
      "======> Epoch: 3 Average Loss: 45.5459\n",
      "Train Epoch: 3 [40000/60000 (67%)]\t Loss: 85.619004\n",
      "======> Epoch: 3 Average Loss: 60.5402\n",
      "Train Epoch: 3 [50000/60000 (83%)]\t Loss: 86.235449\n",
      "======> Epoch: 3 Average Loss: 75.4382\n",
      "\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\t Loss: 90.473613\n",
      "======> Epoch: 4 Average Loss: 0.1508\n",
      "Train Epoch: 4 [10000/60000 (17%)]\t Loss: 91.254355\n",
      "======> Epoch: 4 Average Loss: 14.9244\n",
      "Train Epoch: 4 [20000/60000 (33%)]\t Loss: 87.875088\n",
      "======> Epoch: 4 Average Loss: 29.6801\n",
      "Train Epoch: 4 [30000/60000 (50%)]\t Loss: 89.715244\n",
      "======> Epoch: 4 Average Loss: 44.3370\n",
      "Train Epoch: 4 [40000/60000 (67%)]\t Loss: 87.129531\n",
      "======> Epoch: 4 Average Loss: 59.0232\n",
      "Train Epoch: 4 [50000/60000 (83%)]\t Loss: 86.694619\n",
      "======> Epoch: 4 Average Loss: 73.6656\n",
      "\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\t Loss: 85.121680\n",
      "======> Epoch: 5 Average Loss: 0.1419\n",
      "Train Epoch: 5 [10000/60000 (17%)]\t Loss: 88.101025\n",
      "======> Epoch: 5 Average Loss: 14.6180\n",
      "Train Epoch: 5 [20000/60000 (33%)]\t Loss: 88.374531\n",
      "======> Epoch: 5 Average Loss: 29.0848\n",
      "Train Epoch: 5 [30000/60000 (50%)]\t Loss: 86.030742\n",
      "======> Epoch: 5 Average Loss: 43.5895\n",
      "Train Epoch: 5 [40000/60000 (67%)]\t Loss: 87.889365\n",
      "======> Epoch: 5 Average Loss: 58.0075\n",
      "Train Epoch: 5 [50000/60000 (83%)]\t Loss: 85.653643\n",
      "======> Epoch: 5 Average Loss: 72.3508\n",
      "\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\t Loss: 85.903379\n",
      "======> Epoch: 6 Average Loss: 0.1432\n",
      "Train Epoch: 6 [10000/60000 (17%)]\t Loss: 89.375625\n",
      "======> Epoch: 6 Average Loss: 14.4837\n",
      "Train Epoch: 6 [20000/60000 (33%)]\t Loss: 86.195645\n",
      "======> Epoch: 6 Average Loss: 28.7288\n",
      "Train Epoch: 6 [30000/60000 (50%)]\t Loss: 85.510703\n",
      "======> Epoch: 6 Average Loss: 42.9389\n",
      "Train Epoch: 6 [40000/60000 (67%)]\t Loss: 85.692109\n",
      "======> Epoch: 6 Average Loss: 57.1646\n",
      "Train Epoch: 6 [50000/60000 (83%)]\t Loss: 82.323535\n",
      "======> Epoch: 6 Average Loss: 71.3513\n",
      "\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\t Loss: 80.588877\n",
      "======> Epoch: 7 Average Loss: 0.1343\n",
      "Train Epoch: 7 [10000/60000 (17%)]\t Loss: 82.536641\n",
      "======> Epoch: 7 Average Loss: 14.2084\n",
      "Train Epoch: 7 [20000/60000 (33%)]\t Loss: 83.711982\n",
      "======> Epoch: 7 Average Loss: 28.2525\n",
      "Train Epoch: 7 [30000/60000 (50%)]\t Loss: 80.096099\n",
      "======> Epoch: 7 Average Loss: 42.3353\n",
      "Train Epoch: 7 [40000/60000 (67%)]\t Loss: 82.197051\n",
      "======> Epoch: 7 Average Loss: 56.4086\n",
      "Train Epoch: 7 [50000/60000 (83%)]\t Loss: 83.142813\n",
      "======> Epoch: 7 Average Loss: 70.5136\n",
      "\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\t Loss: 82.548164\n",
      "======> Epoch: 8 Average Loss: 0.1376\n",
      "Train Epoch: 8 [10000/60000 (17%)]\t Loss: 82.977998\n",
      "======> Epoch: 8 Average Loss: 14.0458\n",
      "Train Epoch: 8 [20000/60000 (33%)]\t Loss: 81.716880\n",
      "======> Epoch: 8 Average Loss: 28.0421\n",
      "Train Epoch: 8 [30000/60000 (50%)]\t Loss: 83.931016\n",
      "======> Epoch: 8 Average Loss: 41.9627\n",
      "Train Epoch: 8 [40000/60000 (67%)]\t Loss: 80.889365\n",
      "======> Epoch: 8 Average Loss: 55.8686\n",
      "Train Epoch: 8 [50000/60000 (83%)]\t Loss: 83.533320\n",
      "======> Epoch: 8 Average Loss: 69.7429\n",
      "\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\t Loss: 83.123105\n",
      "======> Epoch: 9 Average Loss: 0.1385\n",
      "Train Epoch: 9 [10000/60000 (17%)]\t Loss: 84.721230\n",
      "======> Epoch: 9 Average Loss: 13.9344\n",
      "Train Epoch: 9 [20000/60000 (33%)]\t Loss: 81.454165\n",
      "======> Epoch: 9 Average Loss: 27.8017\n",
      "Train Epoch: 9 [30000/60000 (50%)]\t Loss: 82.864893\n",
      "======> Epoch: 9 Average Loss: 41.6019\n",
      "Train Epoch: 9 [40000/60000 (67%)]\t Loss: 84.599561\n",
      "======> Epoch: 9 Average Loss: 55.3443\n",
      "Train Epoch: 9 [50000/60000 (83%)]\t Loss: 87.488027\n",
      "======> Epoch: 9 Average Loss: 69.0869\n",
      "\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\t Loss: 83.138066\n",
      "======> Epoch: 10 Average Loss: 0.1386\n",
      "Train Epoch: 10 [10000/60000 (17%)]\t Loss: 81.973105\n",
      "======> Epoch: 10 Average Loss: 13.8394\n",
      "Train Epoch: 10 [20000/60000 (33%)]\t Loss: 84.597676\n",
      "======> Epoch: 10 Average Loss: 27.5305\n",
      "Train Epoch: 10 [30000/60000 (50%)]\t Loss: 81.156440\n",
      "======> Epoch: 10 Average Loss: 41.1960\n",
      "Train Epoch: 10 [40000/60000 (67%)]\t Loss: 80.183027\n",
      "======> Epoch: 10 Average Loss: 54.8533\n",
      "Train Epoch: 10 [50000/60000 (83%)]\t Loss: 80.806641\n",
      "======> Epoch: 10 Average Loss: 68.5389\n",
      "\n",
      "\n",
      "Train Epoch: 11 [0/60000 (0%)]\t Loss: 84.322227\n",
      "======> Epoch: 11 Average Loss: 0.1405\n",
      "Train Epoch: 11 [10000/60000 (17%)]\t Loss: 82.425947\n",
      "======> Epoch: 11 Average Loss: 13.7052\n",
      "Train Epoch: 11 [20000/60000 (33%)]\t Loss: 80.032632\n",
      "======> Epoch: 11 Average Loss: 27.3320\n",
      "Train Epoch: 11 [30000/60000 (50%)]\t Loss: 83.728379\n",
      "======> Epoch: 11 Average Loss: 40.9132\n",
      "Train Epoch: 11 [40000/60000 (67%)]\t Loss: 77.129951\n",
      "======> Epoch: 11 Average Loss: 54.4867\n",
      "Train Epoch: 11 [50000/60000 (83%)]\t Loss: 81.275332\n",
      "======> Epoch: 11 Average Loss: 68.0891\n",
      "\n",
      "\n",
      "Train Epoch: 12 [0/60000 (0%)]\t Loss: 85.265938\n",
      "======> Epoch: 12 Average Loss: 0.1421\n",
      "Train Epoch: 12 [10000/60000 (17%)]\t Loss: 85.226035\n",
      "======> Epoch: 12 Average Loss: 13.5760\n",
      "Train Epoch: 12 [20000/60000 (33%)]\t Loss: 80.810088\n",
      "======> Epoch: 12 Average Loss: 27.1412\n",
      "Train Epoch: 12 [30000/60000 (50%)]\t Loss: 85.154014\n",
      "======> Epoch: 12 Average Loss: 40.6310\n",
      "Train Epoch: 12 [40000/60000 (67%)]\t Loss: 83.855312\n",
      "======> Epoch: 12 Average Loss: 54.1380\n",
      "Train Epoch: 12 [50000/60000 (83%)]\t Loss: 81.209116\n",
      "======> Epoch: 12 Average Loss: 67.6352\n",
      "\n",
      "\n",
      "Train Epoch: 13 [0/60000 (0%)]\t Loss: 84.255166\n",
      "======> Epoch: 13 Average Loss: 0.1404\n",
      "Train Epoch: 13 [10000/60000 (17%)]\t Loss: 84.902480\n",
      "======> Epoch: 13 Average Loss: 13.5563\n",
      "Train Epoch: 13 [20000/60000 (33%)]\t Loss: 79.550273\n",
      "======> Epoch: 13 Average Loss: 26.9938\n",
      "Train Epoch: 13 [30000/60000 (50%)]\t Loss: 81.083115\n",
      "======> Epoch: 13 Average Loss: 40.4364\n",
      "Train Epoch: 13 [40000/60000 (67%)]\t Loss: 76.504346\n",
      "======> Epoch: 13 Average Loss: 53.8265\n",
      "Train Epoch: 13 [50000/60000 (83%)]\t Loss: 80.886367\n",
      "======> Epoch: 13 Average Loss: 67.2929\n",
      "\n",
      "\n",
      "Train Epoch: 14 [0/60000 (0%)]\t Loss: 77.140415\n",
      "======> Epoch: 14 Average Loss: 0.1286\n",
      "Train Epoch: 14 [10000/60000 (17%)]\t Loss: 78.801904\n",
      "======> Epoch: 14 Average Loss: 13.4558\n",
      "Train Epoch: 14 [20000/60000 (33%)]\t Loss: 75.400581\n",
      "======> Epoch: 14 Average Loss: 26.8490\n",
      "Train Epoch: 14 [30000/60000 (50%)]\t Loss: 76.980581\n",
      "======> Epoch: 14 Average Loss: 40.1957\n",
      "Train Epoch: 14 [40000/60000 (67%)]\t Loss: 80.050786\n",
      "======> Epoch: 14 Average Loss: 53.5590\n",
      "Train Epoch: 14 [50000/60000 (83%)]\t Loss: 81.107827\n",
      "======> Epoch: 14 Average Loss: 66.9296\n",
      "\n",
      "\n",
      "Train Epoch: 15 [0/60000 (0%)]\t Loss: 81.188003\n",
      "======> Epoch: 15 Average Loss: 0.1353\n",
      "Train Epoch: 15 [10000/60000 (17%)]\t Loss: 79.553193\n",
      "======> Epoch: 15 Average Loss: 13.4735\n",
      "Train Epoch: 15 [20000/60000 (33%)]\t Loss: 85.545771\n",
      "======> Epoch: 15 Average Loss: 26.7822\n",
      "Train Epoch: 15 [30000/60000 (50%)]\t Loss: 78.737524\n",
      "======> Epoch: 15 Average Loss: 40.0865\n",
      "Train Epoch: 15 [40000/60000 (67%)]\t Loss: 79.006084\n",
      "======> Epoch: 15 Average Loss: 53.3867\n",
      "Train Epoch: 15 [50000/60000 (83%)]\t Loss: 80.858853\n",
      "======> Epoch: 15 Average Loss: 66.7348\n",
      "\n",
      "\n",
      "Train Epoch: 16 [0/60000 (0%)]\t Loss: 76.056235\n",
      "======> Epoch: 16 Average Loss: 0.1268\n",
      "Train Epoch: 16 [10000/60000 (17%)]\t Loss: 83.139023\n",
      "======> Epoch: 16 Average Loss: 13.3721\n",
      "Train Epoch: 16 [20000/60000 (33%)]\t Loss: 79.824023\n",
      "======> Epoch: 16 Average Loss: 26.6522\n",
      "Train Epoch: 16 [30000/60000 (50%)]\t Loss: 77.692085\n",
      "======> Epoch: 16 Average Loss: 39.8859\n",
      "Train Epoch: 16 [40000/60000 (67%)]\t Loss: 81.132554\n",
      "======> Epoch: 16 Average Loss: 53.1565\n",
      "Train Epoch: 16 [50000/60000 (83%)]\t Loss: 78.532485\n",
      "======> Epoch: 16 Average Loss: 66.4246\n",
      "\n",
      "\n",
      "Train Epoch: 17 [0/60000 (0%)]\t Loss: 80.406460\n",
      "======> Epoch: 17 Average Loss: 0.1340\n",
      "Train Epoch: 17 [10000/60000 (17%)]\t Loss: 75.798755\n",
      "======> Epoch: 17 Average Loss: 13.3317\n",
      "Train Epoch: 17 [20000/60000 (33%)]\t Loss: 76.353335\n",
      "======> Epoch: 17 Average Loss: 26.5835\n",
      "Train Epoch: 17 [30000/60000 (50%)]\t Loss: 80.352290\n",
      "======> Epoch: 17 Average Loss: 39.7953\n",
      "Train Epoch: 17 [40000/60000 (67%)]\t Loss: 77.871289\n",
      "======> Epoch: 17 Average Loss: 52.9800\n",
      "Train Epoch: 17 [50000/60000 (83%)]\t Loss: 77.832373\n",
      "======> Epoch: 17 Average Loss: 66.1931\n",
      "\n",
      "\n",
      "Train Epoch: 18 [0/60000 (0%)]\t Loss: 79.364897\n",
      "======> Epoch: 18 Average Loss: 0.1323\n",
      "Train Epoch: 18 [10000/60000 (17%)]\t Loss: 77.475039\n",
      "======> Epoch: 18 Average Loss: 13.3995\n",
      "Train Epoch: 18 [20000/60000 (33%)]\t Loss: 79.584121\n",
      "======> Epoch: 18 Average Loss: 26.6073\n",
      "Train Epoch: 18 [30000/60000 (50%)]\t Loss: 78.494092\n",
      "======> Epoch: 18 Average Loss: 39.7860\n",
      "Train Epoch: 18 [40000/60000 (67%)]\t Loss: 77.904106\n",
      "======> Epoch: 18 Average Loss: 52.9014\n",
      "Train Epoch: 18 [50000/60000 (83%)]\t Loss: 80.411855\n",
      "======> Epoch: 18 Average Loss: 66.1047\n",
      "\n",
      "\n",
      "Train Epoch: 19 [0/60000 (0%)]\t Loss: 79.002861\n",
      "======> Epoch: 19 Average Loss: 0.1317\n",
      "Train Epoch: 19 [10000/60000 (17%)]\t Loss: 76.486377\n",
      "======> Epoch: 19 Average Loss: 13.2698\n",
      "Train Epoch: 19 [20000/60000 (33%)]\t Loss: 78.560293\n",
      "======> Epoch: 19 Average Loss: 26.4095\n",
      "Train Epoch: 19 [30000/60000 (50%)]\t Loss: 81.899419\n",
      "======> Epoch: 19 Average Loss: 39.5276\n",
      "Train Epoch: 19 [40000/60000 (67%)]\t Loss: 76.657437\n",
      "======> Epoch: 19 Average Loss: 52.6942\n",
      "Train Epoch: 19 [50000/60000 (83%)]\t Loss: 78.161309\n",
      "======> Epoch: 19 Average Loss: 65.8530\n",
      "\n",
      "\n",
      "Train Epoch: 20 [0/60000 (0%)]\t Loss: 77.225742\n",
      "======> Epoch: 20 Average Loss: 0.1287\n",
      "Train Epoch: 20 [10000/60000 (17%)]\t Loss: 77.150010\n",
      "======> Epoch: 20 Average Loss: 13.2131\n",
      "Train Epoch: 20 [20000/60000 (33%)]\t Loss: 79.794028\n",
      "======> Epoch: 20 Average Loss: 26.2895\n",
      "Train Epoch: 20 [30000/60000 (50%)]\t Loss: 80.455166\n",
      "======> Epoch: 20 Average Loss: 39.4536\n",
      "Train Epoch: 20 [40000/60000 (67%)]\t Loss: 76.950806\n",
      "======> Epoch: 20 Average Loss: 52.5502\n",
      "Train Epoch: 20 [50000/60000 (83%)]\t Loss: 78.374404\n",
      "======> Epoch: 20 Average Loss: 65.6906\n",
      "\n",
      "\n",
      "Train Epoch: 21 [0/60000 (0%)]\t Loss: 77.825210\n",
      "======> Epoch: 21 Average Loss: 0.1297\n",
      "Train Epoch: 21 [10000/60000 (17%)]\t Loss: 77.758198\n",
      "======> Epoch: 21 Average Loss: 13.1731\n",
      "Train Epoch: 21 [20000/60000 (33%)]\t Loss: 78.467676\n",
      "======> Epoch: 21 Average Loss: 26.2898\n",
      "Train Epoch: 21 [30000/60000 (50%)]\t Loss: 79.294624\n",
      "======> Epoch: 21 Average Loss: 39.3654\n",
      "Train Epoch: 21 [40000/60000 (67%)]\t Loss: 79.461318\n",
      "======> Epoch: 21 Average Loss: 52.5165\n",
      "Train Epoch: 21 [50000/60000 (83%)]\t Loss: 80.831440\n",
      "======> Epoch: 21 Average Loss: 65.5952\n",
      "\n",
      "\n",
      "Train Epoch: 22 [0/60000 (0%)]\t Loss: 74.582168\n",
      "======> Epoch: 22 Average Loss: 0.1243\n",
      "Train Epoch: 22 [10000/60000 (17%)]\t Loss: 78.880391\n",
      "======> Epoch: 22 Average Loss: 13.1735\n",
      "Train Epoch: 22 [20000/60000 (33%)]\t Loss: 77.643862\n",
      "======> Epoch: 22 Average Loss: 26.2688\n",
      "Train Epoch: 22 [30000/60000 (50%)]\t Loss: 82.941709\n",
      "======> Epoch: 22 Average Loss: 39.3846\n",
      "Train Epoch: 22 [40000/60000 (67%)]\t Loss: 80.886064\n",
      "======> Epoch: 22 Average Loss: 52.4397\n",
      "Train Epoch: 22 [50000/60000 (83%)]\t Loss: 78.480117\n",
      "======> Epoch: 22 Average Loss: 65.4475\n",
      "\n",
      "\n",
      "Train Epoch: 23 [0/60000 (0%)]\t Loss: 78.150723\n",
      "======> Epoch: 23 Average Loss: 0.1303\n",
      "Train Epoch: 23 [10000/60000 (17%)]\t Loss: 77.166943\n",
      "======> Epoch: 23 Average Loss: 13.1084\n",
      "Train Epoch: 23 [20000/60000 (33%)]\t Loss: 75.355112\n",
      "======> Epoch: 23 Average Loss: 26.1707\n",
      "Train Epoch: 23 [30000/60000 (50%)]\t Loss: 78.997607\n",
      "======> Epoch: 23 Average Loss: 39.2646\n",
      "Train Epoch: 23 [40000/60000 (67%)]\t Loss: 78.906934\n",
      "======> Epoch: 23 Average Loss: 52.2599\n",
      "Train Epoch: 23 [50000/60000 (83%)]\t Loss: 80.557236\n",
      "======> Epoch: 23 Average Loss: 65.3204\n",
      "\n",
      "\n",
      "Train Epoch: 24 [0/60000 (0%)]\t Loss: 78.855879\n",
      "======> Epoch: 24 Average Loss: 0.1314\n",
      "Train Epoch: 24 [10000/60000 (17%)]\t Loss: 77.681343\n",
      "======> Epoch: 24 Average Loss: 13.1147\n",
      "Train Epoch: 24 [20000/60000 (33%)]\t Loss: 80.073042\n",
      "======> Epoch: 24 Average Loss: 26.1534\n",
      "Train Epoch: 24 [30000/60000 (50%)]\t Loss: 79.498008\n",
      "======> Epoch: 24 Average Loss: 39.1789\n",
      "Train Epoch: 24 [40000/60000 (67%)]\t Loss: 79.500439\n",
      "======> Epoch: 24 Average Loss: 52.1849\n",
      "Train Epoch: 24 [50000/60000 (83%)]\t Loss: 76.467119\n",
      "======> Epoch: 24 Average Loss: 65.1873\n",
      "\n",
      "\n",
      "Train Epoch: 25 [0/60000 (0%)]\t Loss: 74.330659\n",
      "======> Epoch: 25 Average Loss: 0.1239\n",
      "Train Epoch: 25 [10000/60000 (17%)]\t Loss: 79.320264\n",
      "======> Epoch: 25 Average Loss: 13.1200\n",
      "Train Epoch: 25 [20000/60000 (33%)]\t Loss: 74.396162\n",
      "======> Epoch: 25 Average Loss: 26.0486\n",
      "Train Epoch: 25 [30000/60000 (50%)]\t Loss: 76.367495\n",
      "======> Epoch: 25 Average Loss: 39.0410\n",
      "Train Epoch: 25 [40000/60000 (67%)]\t Loss: 77.782197\n",
      "======> Epoch: 25 Average Loss: 52.0622\n",
      "Train Epoch: 25 [50000/60000 (83%)]\t Loss: 80.729258\n",
      "======> Epoch: 25 Average Loss: 65.0766\n",
      "\n",
      "\n",
      "Train Epoch: 26 [0/60000 (0%)]\t Loss: 77.071445\n",
      "======> Epoch: 26 Average Loss: 0.1285\n",
      "Train Epoch: 26 [10000/60000 (17%)]\t Loss: 77.802490\n",
      "======> Epoch: 26 Average Loss: 13.0297\n",
      "Train Epoch: 26 [20000/60000 (33%)]\t Loss: 77.006797\n",
      "======> Epoch: 26 Average Loss: 26.0184\n",
      "Train Epoch: 26 [30000/60000 (50%)]\t Loss: 78.137705\n",
      "======> Epoch: 26 Average Loss: 38.9341\n",
      "Train Epoch: 26 [40000/60000 (67%)]\t Loss: 77.669175\n",
      "======> Epoch: 26 Average Loss: 51.9618\n",
      "Train Epoch: 26 [50000/60000 (83%)]\t Loss: 81.302451\n",
      "======> Epoch: 26 Average Loss: 64.9428\n",
      "\n",
      "\n",
      "Train Epoch: 27 [0/60000 (0%)]\t Loss: 81.347520\n",
      "======> Epoch: 27 Average Loss: 0.1356\n",
      "Train Epoch: 27 [10000/60000 (17%)]\t Loss: 76.936846\n",
      "======> Epoch: 27 Average Loss: 13.0290\n",
      "Train Epoch: 27 [20000/60000 (33%)]\t Loss: 81.338052\n",
      "======> Epoch: 27 Average Loss: 25.9859\n",
      "Train Epoch: 27 [30000/60000 (50%)]\t Loss: 80.921621\n",
      "======> Epoch: 27 Average Loss: 38.9007\n",
      "Train Epoch: 27 [40000/60000 (67%)]\t Loss: 77.306650\n",
      "======> Epoch: 27 Average Loss: 51.8445\n",
      "Train Epoch: 27 [50000/60000 (83%)]\t Loss: 79.138032\n",
      "======> Epoch: 27 Average Loss: 64.8531\n",
      "\n",
      "\n",
      "Train Epoch: 28 [0/60000 (0%)]\t Loss: 78.092119\n",
      "======> Epoch: 28 Average Loss: 0.1302\n",
      "Train Epoch: 28 [10000/60000 (17%)]\t Loss: 76.812422\n",
      "======> Epoch: 28 Average Loss: 13.0431\n",
      "Train Epoch: 28 [20000/60000 (33%)]\t Loss: 76.486792\n",
      "======> Epoch: 28 Average Loss: 25.9500\n",
      "Train Epoch: 28 [30000/60000 (50%)]\t Loss: 75.978643\n",
      "======> Epoch: 28 Average Loss: 38.8449\n",
      "Train Epoch: 28 [40000/60000 (67%)]\t Loss: 76.274175\n",
      "======> Epoch: 28 Average Loss: 51.7519\n",
      "Train Epoch: 28 [50000/60000 (83%)]\t Loss: 76.641704\n",
      "======> Epoch: 28 Average Loss: 64.7024\n",
      "\n",
      "\n",
      "Train Epoch: 29 [0/60000 (0%)]\t Loss: 74.751401\n",
      "======> Epoch: 29 Average Loss: 0.1246\n",
      "Train Epoch: 29 [10000/60000 (17%)]\t Loss: 78.994648\n",
      "======> Epoch: 29 Average Loss: 12.9958\n",
      "Train Epoch: 29 [20000/60000 (33%)]\t Loss: 74.327568\n",
      "======> Epoch: 29 Average Loss: 25.8928\n",
      "Train Epoch: 29 [30000/60000 (50%)]\t Loss: 80.283413\n",
      "======> Epoch: 29 Average Loss: 38.8365\n",
      "Train Epoch: 29 [40000/60000 (67%)]\t Loss: 80.944365\n",
      "======> Epoch: 29 Average Loss: 51.7641\n",
      "Train Epoch: 29 [50000/60000 (83%)]\t Loss: 76.511226\n",
      "======> Epoch: 29 Average Loss: 64.7025\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "for epoch in tqdm(range(0, epochs)):\n",
    "  train(epoch, model, train_loader, optimizer)\n",
    "  test(epoch, model, test_loader)\n",
    "  print(\"\\n\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate images from test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc117e8e2dec48c3b817a1e6c20ecd40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (x, _) in enumerate(tqdm(test_loader)):\n",
    "        x = x.view(batch_size, x_dim)\n",
    "        x = x.to(device)\n",
    "        \n",
    "        x_hat, _, _ = model(x)\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(x, idx):\n",
    "    x = x.view(batch_size, 28, 28)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.imshow(x[idx].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image(x, idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image(x_hat, idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-78e4cd337088f98f\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-78e4cd337088f98f\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./tensorboard_scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
